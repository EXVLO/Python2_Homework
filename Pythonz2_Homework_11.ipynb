{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n",
      "Shapes: (1000, 3) (3,)\n",
      "Initial gradient w: [-4.61089207  7.28473268 -3.01277561]\n",
      "Initial gradient b: -10.38398487762408\n"
     ]
    }
   ],
   "source": [
    "# IN-CLASS TASK:\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "\n",
    "\n",
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    return X @ w + b\n",
    "\n",
    "\n",
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    diff = y_pred - y_true\n",
    "    return float(np.mean(diff ** 2))\n",
    "\n",
    "\n",
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    n_samples = X.shape[0]\n",
    "    error = y_pred - y\n",
    "    gradient_w = (2.0 / n_samples) * (X.T @ error)\n",
    "    gradient_b = (2.0 / n_samples) * np.sum(error)\n",
    "    return gradient_w, float(gradient_b)\n",
    "\n",
    "\n",
    "w = np.zeros(n_features)\n",
    "b = 0.0\n",
    "\n",
    "print(\"Shapes:\", X.shape, w.shape)\n",
    "\n",
    "y_pred = predict(X, w, b)\n",
    "grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "\n",
    "print(\"Initial gradient w:\", grad_w)\n",
    "print(\"Initial gradient b:\", grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
